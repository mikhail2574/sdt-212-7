# Branching LangGraph Agent — Submission

## Scenario

**Quick Research Buddy** (search → cite → answer) + `calc` + `remember` memory extraction.

The agent routes inputs via an LLM planner/router to one of three tools, loops planner→tool→planner, and then composes a final answer using scratchpad notes and persisted profile memory.

---

## Diagram (Mermaid)

```mermaid
graph TD
  A([start]) --> B[ingest user_input]
  B --> C[planner/router (LLM)]
  C -->|next=search| D[tool: wikipedia_summary]
  C -->|next=calc| E[tool: safe_calc]
  C -->|next=remember| F[tool: remember_facts (LLM)]
  C -->|next=final| G[final_answer (LLM)]
  D --> C
  E --> C
  F --> C
  G --> H([end])

  subgraph Memory
    M[InMemorySaver + thread_id]
  end
```

---

## Graph Description

### Nodes

- **ingest**: appends the new user message to `messages`, resets `scratchpad`, sets `step=0` for the turn.
- **planner** (LLM): outputs JSON decision:
  - `{"next":"search|calc|remember|final","tool_input":"...","reason":"..."}`
  - includes step-cap guardrail (`MAX_STEPS`, default 3).
- **search**: Wikipedia REST summary tool.
- **calc**: safe arithmetic via AST allowlist (no `eval`).
- **remember**: LLM-based profile extractor → merges facts into `profile` dict (persisted memory).
- **final** (LLM): writes the final response using `profile` + tool scratchpad notes.

### Tools

1. **Wikipedia summary**  
   GET `https://en.wikipedia.org/api/rest_v1/page/summary/{title}`

2. **Calculator (safe_calc)**  
   Sanitized characters + AST allowlist; supports `^` as exponent (mapped to `**`).

3. **Remember (fact extraction)**  
   LLM extraction into JSON `{ "facts": { ... } }` using OpenRouter.

### Memory

- `InMemorySaver` checkpointer with `thread_id` ensures state persists across turns.
- Profile facts are stored in `state["profile"]`.

### Scratchpad

- Each tool appends structured notes to `state["scratchpad"]`.
- Final node uses scratchpad notes and then clears scratchpad at the end of the turn.

---

## Environment

### APIs & Models

- LLM: OpenRouter chat completions
- Default model: `OPENROUTER_MODEL=openai/gpt-4o-mini`

### Required keys (do not include values)

- `OPENROUTER_API_KEY`

### Optional

- `OPENROUTER_MODEL`
- `OPENROUTER_APP_URL`
- `OPENROUTER_APP_NAME`
- `MAX_STEPS`

---

## Routing Policy (Planner)

- **remember**: user states stable personal facts/preferences (name, city, preferences).
- **calc**: user requests arithmetic computation.
- **search**: user asks world-knowledge question likely requiring external facts.
- **final**: can answer from memory/profile or existing scratchpad; also used when step cap is reached.

Guardrails:

- strict JSON-only output (planner prompt)
- parse + validate (Pydantic) + retry once if invalid JSON
- heuristic fallback if parsing still fails
- step cap to prevent infinite loops and control cost

---

## 10 Test Inputs + Logs

Generated by:

```bash
python scripts/capture_demo.py --thread-id demo
```

Output:

- `artifacts/demo_run.md` (planner decisions, tool notes, final answers)

---

## Reflection

Compared to a single prompt baseline, the planner+tools loop improves factual accuracy and transparency. The planner correctly routes arithmetic to a safe calculator (so the LLM does not hallucinate math). For factual questions, the search tool provides grounded Wikipedia summaries and the final node can cite the exact Wikipedia page URL. Memory extraction enables personalization and continuity across turns (e.g., "What's my name?" becomes a direct recall from `profile` rather than a guess).

Main failure cases involved routing mistakes and malformed JSON from the planner. Mitigations: enforced JSON schema, retries, and a strict step cap; heuristic fallback routing when parsing fails; calculator sanitization prevents arbitrary code execution and constrains expression complexity. Trade-off: strict parsing and step limits may sometimes force an early "final" answer, but it prevents infinite loops and reduces cost.

---

## Bonus: Guardrails & Human-in-the-Loop (implemented)

### Retry / backoff

The OpenRouter client implements retry with exponential backoff for transient failures (network/5xx/429). This improves robustness in demos and reduces flakiness.

### Human-in-the-loop approval

For memory updates, an optional approval mode can be enabled (e.g., env `HITL_MEMORY_APPROVAL=1`) where extracted facts are staged as `pending_facts` and only committed to `profile` after explicit user confirmation (“approve memory update”).  
Trade-off: improves safety (prevents storing wrong facts) but adds an extra turn and slightly reduces UX smoothness.
